[link](https://www.youtube.com/watch?v=k87SPgq-me4)

### what does a product manager do?

- ideates: design new products, services, features, and improvements
- priortizes: which ideas to build and implement?
- persuades: convince leadership, engineering, and other stakeholders
- executes: build and launch products, track, and manage performance

### what is the interviewer looking for?

- Can you identify what data is relavant?
- Can you correctly interpret data to make decisions?

- Can you use data to persuade others?
- Are you realistic about what data can be collected?
- Can you identify the levers that can be controlled?

### I group product analysis into four buckets (with example questions)

- Opportunity sizing
  - should we launch product/feature X?
  - How many italian restaurants are in seattle?
- Prioritization
  - Given X amount of time/money, which ideas should be prioritized?
  - Of all the features we discussed, which one would you build?
- Success definition and measurement
  - Pick one metric to manage the feature you designed?
  - How will you measure success after you launch your idea?
- Diagnosing issues
  - Ad revenue dropped by 20%, how will you identify the issue?
  - Why are 'Product Page Views' down?


- As a prouduct manager, you are really responsible for two things, A you have to know what game you're playing and B you have to know how you score.


### Product analysis: Opportunity sizing
- Clarify scope
- Identify the base population (how many ppl have access to this service)
- Identify addressable segments? (how many ppl are actually going to use this service - target segments)
- Identify purchase frequency
- Identify purchase price

### How well do you understand 'the funnel'?
- 100% Total site visitors
  - Input: marketing, SEO, seasonality
- 70% view item detail page
  - Input: #items shown, Relevance, Price, UX quality
- 20% hit 'add to cart' button
  - Input: Product description, # of images, Product rating and reviews
- 7% go to 'checkout'
  - Input: Calls to action, Promotions
- 3% complete purchase
  - Input: Card on file, # of fields to complete, registered user VS guest

- This example dipicts a simple ecommerce website's funnel

### Prioritize for impact

- You have to understand what you are impacting before you come up with that. So, the steps really are what are the features how much are they going to impact the metric that matters, what metric is the most important in this stage of the company.
- Identify what is the metric that you're optimizing for. which one of your features that you describe hit that metric the most?

- The funnel varies by segment, source of traffic etc.
- Troubleshooting: identify the problematic step and narrow down possible causes.

### A/B tests

- A 'Control' group of users is presented with an experience 'A' that is unchanged from the status quo
- A 'Treatment' group is presented with an alternate experience 'B'
- The behavior of these two groups is compared over time.
- select a test metric -> calculate sample size -> run test -> analyze results.

#### Common mistakes with A/B tests

##### picking the wrong test metric
- Financial metrics such as 'revenue per order', 'profit' etc. have a very high natural degree of variance. These make lousy candidates for A/B testing.
- When variance is high, you need a huge sample size. Tests can run for years! speed is key, nobody has time for this. 
- Tip: pick low variance metrics such as 'order per user', 'click thru rate' etc. Click thrus and conversion are more telling anyway.

##### Misunderstanding 'Statistical Significance'
- You do not run a test until it shows statistical significance. You run an A/B test until you hit the required sample size and then you check the result.
- At that point 1) there is a statistically significant difference between control and treatment. 2)There is no statistically significant difference between control and treatment.
- Tip: use Evan miller's A/B testing blog for calculators.

#####  'peeking' at results to draw 'early data'
- Do not try to read into results until sample size is reached.
- We plotted the results of 100 experiments. Even though ~60% of them showed significance at some point, only ONE had a statistically significant difference after sample sizes were reached.

##### ingore 'seasonality'

- On a high traffic website (think Netflix, Google, Amazon), I may reach sample size in a few hours.
- But does that give me the full picture?
- Users may behave differently over weekends. Make sure you run the test long enough to capture nuances.



- A pre/post analysis is a messy but acceptable last-resort alternative.
- i.e. measure performance before and after the change, accounting for other impacts.

- It is wrong to suppose that if you can't measure it, you can't manage it - a costly myth
- You can't not measure everything, you should be as data-driven as possible but beyond that you have to basically just manage the things you can without the data and to be honest product management is basically you are running a business it's a lot about risk taking. You can't really wait for all the data you have to use a law here and one intuition you have to use your judgement which is why I keep emphasizing on the logic and the apporoach is more important than the actual data itself.


